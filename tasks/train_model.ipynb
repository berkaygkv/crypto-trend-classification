{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c13b58",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147207ff",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model = \"xgboost.XGBClassifier\"\n",
    "threshold = 0.5\n",
    "eval_metric = \"auc\"\n",
    "objective = \"binary:logistic\"\n",
    "params_names = [\"threshold\", \"objective\", \"eval_metric\"]\n",
    "track = False\n",
    "mlflow_tracking_uri = (\n",
    "    \"file:C:\\\\Users\\\\berkayg\\\\Desktop\\\\Coding env\\\\crypto-prediction-project/mlruns\"\n",
    ")\n",
    "upstream = {\n",
    "    \"data_processing\": {\n",
    "        \"nb\": \"C:\\\\Users\\\\berkayg\\\\Desktop\\\\Coding env\\\\crypto-prediction-project\\\\products\\\\notebooks\\\\process_data.ipynb\",\n",
    "        \"data_train\": \"C:\\\\Users\\\\berkayg\\\\Desktop\\\\Coding env\\\\crypto-prediction-project\\\\products\\\\data\\\\processed_train_data.csv\",\n",
    "        \"data_validation\": \"C:\\\\Users\\\\berkayg\\\\Desktop\\\\Coding env\\\\crypto-prediction-project\\\\products\\\\data\\\\processed_validation_data.csv\",\n",
    "    }\n",
    "}\n",
    "product = {\n",
    "    \"nb\": \"C:\\\\Users\\\\berkayg\\\\Desktop\\\\Coding env\\\\crypto-prediction-project\\\\products\\\\notebooks\\\\report-0.ipynb\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler as RUS\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "import atexit\n",
    "import importlib\n",
    "import mlflow\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.figsize\"] = (11,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "model_params = {k: globals()[k] for k in params_names}\n",
    "print(model_params)\n",
    "\n",
    "# %%\n",
    "if track:\n",
    "    print('tracking with mlflow...')\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "    @atexit.register\n",
    "    def end_run():\n",
    "        mlflow.end_run()\n",
    "else:\n",
    "    print('tracking skipped...')\n",
    "    mlflow = Mock()\n",
    "\n",
    "# %%\n",
    "module, _, class_name = model.rpartition('.')\n",
    "Class_ = getattr(importlib.import_module(module), class_name)\n",
    "Class_\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(name=class_name)\n",
    "except MlflowException:\n",
    "    experiment_id = mlflow.get_experiment_by_name(name=class_name).experiment_id\n",
    "\n",
    "print(f'experiment id: {experiment_id}')\n",
    "\n",
    "# %%\n",
    "run = mlflow.start_run(experiment_id=experiment_id)\n",
    "\n",
    "# %% tags=[\"mlflow-run-id\"]\n",
    "print(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_csv(path, parse_dates=[0], index_col=[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_data(upstream[\"data_processing\"][\"data_train\"])\n",
    "df_validation = read_data(upstream[\"data_processing\"][\"data_validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = df_train.action.value_counts()[0] / df_train.action.value_counts()[1]\n",
    "print(f\"Class Weight: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=10)\n",
    "X = df_train.drop(columns=['action'], axis=1)\n",
    "y = df_train['action']#.map(action_dictionary)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "featureScores = featureScores.sort_values(\"Score\", ascending=False).reset_index(drop=True)\n",
    "features = featureScores.head(10).Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['action'], axis=1)\n",
    "# X = X[features]\n",
    "y = df_train['action']  # .map(action_dictionary)\n",
    "\n",
    "X_valid = df_validation.drop(columns=['action'], axis=1)\n",
    "# X_valid = X_valid[features]\n",
    "y_valid = df_validation['action']  # .map(action_dictionary)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=11)\n",
    "\n",
    "\n",
    "scaler_whitelist = [\"Price\", \"ph\", \"pl\"]\n",
    "for col in X.columns.drop(scaler_whitelist):\n",
    "    trans = StandardScaler()\n",
    "    X_train[col] = trans.fit_transform(X_train[col].values.reshape(-1, 1))\n",
    "    X_test[col] = trans.transform(X_test[col].values.reshape(-1, 1))\n",
    "    X_valid[col] = trans.transform(X_valid[col].values.reshape(-1, 1))\n",
    "\n",
    "smote = SMOTE(random_state=11)\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=3,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=11)\n",
    "\n",
    "xgcls = xgb.XGBClassifier(use_label_encoder=False)\n",
    "param_grid = {\n",
    "    \"reg_alpha\": [0.01, 0.5, 2, 0.2],\n",
    "    \"scale_pos_weight\": [class_weight],\n",
    "    \"learning_rate\": [0.1, 0.2, 0.15],\n",
    "    \"eval_metric\": [eval_metric],\n",
    "    \"objective\": [objective]\n",
    "}\n",
    "if not eval_metric:\n",
    "    param_grid.pop(\"eval_metric\")\n",
    "\n",
    "dummy_cls = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_cls.fit(X_train, y_train)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgcls,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=\"f1\",\n",
    "                           cv=stratified_kfold,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "cv_score = grid_search.best_score_\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_.copy()\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = xgb.XGBClassifier(use_label_encoder=False, **best_params)\n",
    "cls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = cls.get_booster().get_score(importance_type='gain')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "data.nlargest(40, columns=\"score\").sort_values(by = \"score\", ascending=True).plot(kind='barh', figsize = (20,10)) ## plot top 40 features\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stratified K-Fold cross validation\n",
    "def cross_val_model(model, X_test, y_test):\n",
    "    scoring = ('f1', 'recall', 'precision', 'roc_auc')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=1)\n",
    "    scores = cross_validate(model, X_test, y_test, scoring=scoring, cv=cv)\n",
    "\n",
    "    # Obtain model scores\n",
    "    print('Mean f1: %.3f' % scores['test_f1'].mean())\n",
    "    print('Mean recall: %.3f' % scores['test_recall'].mean())\n",
    "    print('Mean precision: %.3f' % scores['test_precision'].mean())\n",
    "    print('Mean auc: %.3f' % scores['test_roc_auc'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossvalidate Test Data\n",
    "cross_val_model(cls, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "y_scores = cls.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "test_set_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "Markdown(f\"\"\"#### Confusion Matrix\n",
    "\n",
    "__Applied Threshold__: {threshold}\n",
    "\"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cls.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig_cmp, ax = plt.subplots()\n",
    "cmp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "#    display_labels=[\"class_1\", \"class_2\", \"class_3\", ],\n",
    ")\n",
    "\n",
    "cmp.plot(ax=ax)\n",
    "plt.grid(False)\n",
    "plt.title(f\"Confusion Matrix (Threshold = {threshold})\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dummy_cls.predict(X_test)\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "dummy_fig_cmp, ax = plt.subplots()\n",
    "cmp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "#    display_labels=[\"class_1\", \"class_2\", \"class_3\", ],\n",
    ")\n",
    "\n",
    "cmp.plot(ax=ax)\n",
    "plt.grid(False)\n",
    "plt.title(f\"Confusion Matrix (DUMMY)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roc Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "# predict probabilities\n",
    "yhat = cls.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_test, yhat)\n",
    "# plot the roc curve for the model\n",
    "fig_roc, ax = plt.subplots()\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='XGBClassifier')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.title(f\"ROC Curve (Threshold = {threshold})\")\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data\n",
    "y_scores = cls.predict_proba(X_valid)[:, 1]\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weighted = xgb.XGBClassifier(**best_params)\n",
    "model_weighted.fit(X_train, y_train)\n",
    "#cross_val_model(model_weighted, X_train, y_train)\n",
    "y_pred = model_weighted.predict(X_test)\n",
    "y_scores = model_weighted.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under = RUS(sampling_strategy=1,random_state=4)\n",
    "X_train_under, y_train_under = under.fit_resample(X_train, y_train)\n",
    "\n",
    "best_params_under = best_params.copy()\n",
    "best_params_under.pop(\"scale_pos_weight\")\n",
    "model_weighted = xgb.XGBClassifier(**best_params_under)\n",
    "model_weighted.fit(X_train_under, y_train_under)\n",
    "\n",
    "#cross_val_model(model_weighted, X_train_under, y_train_under)\n",
    "y_scores = model_weighted.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy=\"minority\", random_state=4)\n",
    "X_train_smote, y_train_smote = over.fit_resample(X_train, y_train)\n",
    "\n",
    "best_params_under = best_params.copy()\n",
    "best_params_under.pop(\"scale_pos_weight\")\n",
    "model_weighted = xgb.XGBClassifier(**best_params_under)\n",
    "model_weighted.fit(X_train_smote, y_train_smote)\n",
    "#cross_val_model(model_weighted, X_test, y_test)\n",
    "y_scores = model_weighted.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "bc = SMOTETomek(random_state=4)\n",
    "X_resampled, y_resampled = bc.fit_resample(X_train, y_train)\n",
    "\n",
    "best_params_under = best_params.copy()\n",
    "best_params_under.pop(\"scale_pos_weight\")\n",
    "model_weighted = xgb.XGBClassifier(**best_params_under)\n",
    "model_weighted.fit(X_resampled, y_resampled)\n",
    "#cross_val_model(model_weighted, X_test, y_test)\n",
    "y_scores = model_weighted.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_scores >= threshold).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Optimal Threshold Value is:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_table(test_x, test_y, threshold=0.5):\n",
    "    y_prob = cls.predict_proba(test_x)#[:, 1]\n",
    "    #df = pd.DataFrame([y_prob.argmax(axis=1), y_prob.max(axis=1)]).T\n",
    "    df = pd.DataFrame([y_prob.argmax(axis=1), y_prob[:, 1]]).T\n",
    "    df.columns = [\"prediction\", \"probability\"]\n",
    "    df = pd.concat([test_x.reset_index(drop=True), df], ignore_index=False, axis=1)\n",
    "    df[\"prediction\"] = np.where(df[\"probability\"] >= threshold, 1, 0)\n",
    "    df.index = test_x.index\n",
    "    df[\"y_true\"] = test_y\n",
    "    df[\"dummy_prediction\"] = df[\"prediction\"].sample(frac=1).values\n",
    "    df[\"Close\"] = df_validation[\"Close\"] # df_validation must be parameterized\n",
    "    df[\"return\"] = df[\"Close\"].pct_change()\n",
    "    # df[\"prediction_group\"] = (df[\"prediction\"] == 1).cumsum()\n",
    "    # df[\"dummy_prediction_group\"] = (df[\"dummy_prediction\"] == 1).cumsum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = create_result_table(X_valid, y_valid, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(df_source, df, true_prediction=False):\n",
    "    global df_resid\n",
    "    df_resid = pd.concat([df_source, df[[\"prediction\", \"dummy_prediction\"]]], axis=1).dropna().reset_index(drop=True)\n",
    "    #return df_resid\n",
    "    prediction_residual_mean = np.nan\n",
    "    if true_prediction:\n",
    "        prediction_residual_mean = df_resid.query(\"prediction == 1\").apply(resid, axis=1).mean()\n",
    "    dummy_residual_mean = df_resid.query(\"dummy_prediction == 1\").apply(resid, axis=1).mean()\n",
    "    return prediction_residual_mean, dummy_residual_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_data = {\"Close\": True}\n",
    "\n",
    "fig_obj = px.line(x=\"Date\", y=\"Close\", data_frame=results.sort_index().reset_index(), hover_data=hover_data)\n",
    "\n",
    "extrema = px.scatter(x=\"Date\", y=\"Close\", data_frame=results.sort_index().reset_index().query(\"prediction == 1\"),hover_data=hover_data)\n",
    "extrema.update_traces(marker=dict(color='green'))\n",
    "\n",
    "extrema_dummy = px.scatter(x=\"Date\", y=\"Close\", data_frame=results.sort_index().reset_index().query(\"y_true == 1 and prediction==1\"),hover_data=hover_data)\n",
    "extrema_dummy.update_traces(marker=dict(color='red'))\n",
    "\n",
    "fig = fig_obj.data + extrema.data# + extrema_dummy.data\n",
    "\n",
    "validation_prediction_plot = go.Figure(fig)\n",
    "validation_prediction_plot.update_layout(\n",
    "    title=f\"Validation Graph (Threshold = {threshold})\"\n",
    ")\n",
    "\n",
    "validation_prediction_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid(x):\n",
    "    action_col = df_resid.columns.get_loc(\"action\")\n",
    "    idx = x.name\n",
    "    cumsum_reversed = ((df_resid.iloc[idx::-1, action_col] == 1))\n",
    "    cumsum_reversed = np.where(cumsum_reversed == True)[0]\n",
    "\n",
    "    cumsum_straight = ((df_resid.iloc[idx:, action_col] == 1))\n",
    "    cumsum_straight = np.where(cumsum_straight == True)[0]\n",
    "    \n",
    "    val_straight = np.inf\n",
    "    val_reversed = np.inf\n",
    "    if cumsum_straight.shape[0] > 0:\n",
    "        cumsum_straight = cumsum_straight[0]\n",
    "        val1 = df_resid.iloc[idx + cumsum_straight, df_resid.columns.get_loc(\"Close\")]\n",
    "        val2 = df_resid.iloc[idx, df_resid.columns.get_loc(\"Close\")]\n",
    "        arr2 = np.array([0, val2])\n",
    "        arr1 = np.array([cumsum_straight, val1])\n",
    "        #val_straight = abs(val1 - val2) / min(val1, val2) * 100\n",
    "        euclidean_dist = abs(np.linalg.norm(arr2-arr1))\n",
    "        val_straight = np.sqrt(np.square(euclidean_dist))\n",
    "        val_straight = abs(cumsum_straight)\n",
    "        \n",
    "    if cumsum_reversed.shape[0] > 0:\n",
    "        cumsum_reversed = cumsum_reversed[0]\n",
    "        val1 = df_resid.iloc[idx - cumsum_reversed, df_resid.columns.get_loc(\"Close\")]\n",
    "        val2 = df_resid.iloc[idx, df_resid.columns.get_loc(\"Close\")]\n",
    "        arr2 = np.array([0, val2])\n",
    "        arr1 = np.array([cumsum_reversed, val1])\n",
    "        #val_reversed = abs(val1 - val2) / min(val1, val2) * 100\n",
    "        euclidean_dist = abs(np.linalg.norm(arr2-arr1))\n",
    "        val_reversed = np.sqrt(np.square(euclidean_dist))\n",
    "        val_reversed = abs(cumsum_reversed)\n",
    "        \n",
    "    val = min(val_straight, val_reversed)\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def simulate_loss(df_source, valid_x, valid_y, n=100):\n",
    "    dummy_losses = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        results = create_result_table(valid_x, valid_y)\n",
    "        _, dummy_loss = calculate_loss(df_source, results)\n",
    "        dummy_losses[i] = dummy_loss\n",
    "    return dummy_losses\n",
    "dummy_losses = simulate_loss(df_validation, X_valid, y_valid,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dummy_losses)\n",
    "plt.title(\"Simulated Dummy Loss Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Loss means:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_loss, _ = calculate_loss(df_validation, results, true_prediction=True)\n",
    "prediction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred_loss = (dummy_losses.mean() - prediction_loss) / min(dummy_losses.mean(), prediction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Estimator\n",
    "fig_cr, ax = plt.subplots()\n",
    "visualizer = ClassificationReport(cls, ax, classes=[\"neutral\", \"anomalous\"], support=True)\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.finalize();\n",
    "ax.set_title(ax.get_title() + f\"(Threshold = {threshold})\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Estimator\n",
    "dummu_fig_cr, ax = plt.subplots()\n",
    "visualizer = ClassificationReport(dummy_cls, ax, classes=[\"neutral\", \"anomalous\"], support=True)\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.finalize();\n",
    "ax.set_title(ax.get_title() + \"(DUMMY)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import DiscriminationThreshold\n",
    "fig_disc, ax = plt.subplots()\n",
    "visualizer = DiscriminationThreshold(cls)\n",
    "visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.finalize();\n",
    "ax.set_title(ax.get_title() + f\"(Threshold = {threshold})\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "log_params = cls.get_params().copy()\n",
    "log_params.update({\"threshold\": threshold})\n",
    "mlflow.log_params(log_params)\n",
    "\n",
    "# %%\n",
    "test_set_report_0 = dict((f\"0_{k}\", round(v, 3)) for k, v in test_set_report[\"0.0\"].items())\n",
    "test_set_report_1 = dict((f\"1_{k}\", round(v, 3)) for k, v in test_set_report[\"1.0\"].items())\n",
    "mlflow.log_metrics(test_set_report_0)\n",
    "mlflow.log_metrics(test_set_report_1)\n",
    "mlflow.log_metric(\"dummy_pred_loss\", dummy_pred_loss)\n",
    "\n",
    "\n",
    "mlflow.log_figure(fig_cmp, 'confusion_matrix.png');\n",
    "mlflow.log_figure(fig_cr, 'classification_report.png');\n",
    "mlflow.log_figure(fig_roc, 'roc.png');\n",
    "mlflow.log_figure(fig_disc, 'discrimination_threshold.png');\n",
    "mlflow.log_figure(validation_prediction_plot, 'validation_results.html');\n",
    "mlflow.log_figure(dummy_fig_cmp, 'dummy_confusion_matrix.png')\n",
    "mlflow.log_figure(dummu_fig_cr, 'dummy_classification_report.png')\n",
    "# %%\n",
    "mlflow.sklearn.log_model(cls, artifact_path=\"sklearn-model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7aa3e32ad1ac443041e179702ff775d0d6aade27c9973dce7d8a7f55c4f1197d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "papermill": {
   "parameters": {
    "eval_metric": "auc",
    "mlflow_tracking_uri": "file:C:\\Users\\berkayg\\Desktop\\Coding env\\crypto-prediction-project/mlruns",
    "model": "xgboost.XGBClassifier",
    "objective": "binary:logistic",
    "params_names": [
     "threshold",
     "objective",
     "eval_metric"
    ],
    "product": {
     "nb": "C:\\Users\\berkayg\\Desktop\\Coding env\\crypto-prediction-project\\products\\notebooks\\report-0.ipynb"
    },
    "threshold": 0.5,
    "track": false,
    "upstream": {
     "data_processing": {
      "data_train": "C:\\Users\\berkayg\\Desktop\\Coding env\\crypto-prediction-project\\products\\data\\processed_train_data.csv",
      "data_validation": "C:\\Users\\berkayg\\Desktop\\Coding env\\crypto-prediction-project\\products\\data\\processed_validation_data.csv",
      "nb": "C:\\Users\\berkayg\\Desktop\\Coding env\\crypto-prediction-project\\products\\notebooks\\process_data.ipynb"
     }
    }
   }
  },
  "ploomber": {
   "injected_manually": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
